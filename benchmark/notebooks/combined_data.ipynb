{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaballas/AutoGPT/blob/master/benchmark/notebooks/combined_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZmFethGGbbe",
        "outputId": "227b4a2f-7b11-4066-85da-b95f5b0261cf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from datasets import load_dataset\n",
        "import nltk\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^A-Za-z0-9\\s.,!?]', '', text).lower()\n",
        "    words = text.split()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    return words\n",
        "\n",
        "# Load dataset from Hugging Face\n",
        "dataset = load_dataset(\"Kaballas/HRMIS_MASTER\", split=\"train\")\n",
        "\n",
        "# Preprocess the dataset\n",
        "preprocessed_data = []\n",
        "for example in dataset:\n",
        "    words = preprocess_text(example['questions'] + example['answers'])\n",
        "    preprocessed_data.extend(words)\n",
        "\n",
        "# Now preprocessed_data contains all the preprocessed words from the dataset\n",
        "print(f\"Total preprocessed words: {len(preprocessed_data)}\")\n",
        "print(f\"First 10 words: {preprocessed_data[:10]}\")\n",
        "\n",
        "# Save preprocessed data to a text file\n",
        "output_file = \"preprocessed_data.txt\"\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
        "    for word in preprocessed_data:\n",
        "        file.write(word + \"\\n\")\n",
        "\n",
        "print(f\"Preprocessed data saved to {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrtAgy-0GmfS",
        "outputId": "94fd822b-e398-4c1f-bffc-cc6de52aec48"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total preprocessed words: 89845\n",
            "First 10 words: ['purpose', 'parallel', 'pay', 'run', 'test', 'strategy', 'document?the', 'purpose', 'parallel', 'pay']\n",
            "Preprocessed data saved to preprocessed_data.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "def encode_input(text):\n",
        "    return tokenizer.encode(text, return_tensors='pt')\n",
        "\n",
        "def decode_output(tokens):\n",
        "    return tokenizer.decode(tokens, skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "N1Ef8CMKHlnX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def generate_story_gpt2(seed_text, max_length=100, top_k=50, top_p=0.95):\n",
        "    input_ids = encode_input(seed_text)\n",
        "    sample_outputs = model.generate(\n",
        "        input_ids,\n",
        "        do_sample=True,\n",
        "        max_length=max_length,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "    return decode_output(sample_outputs[0])\n",
        "\n",
        "seed_text = \"What data is used for Playbacks 1 and 2 in the Data environment?\"\n",
        "story = generate_story_gpt2(seed_text)\n",
        "print(story)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIvWdKTsHzNi",
        "outputId": "7bbdb505-0c39-44f6-c998-efa7d2fd10c9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What data is used for Playbacks 1 and 2 in the Data environment?\n",
            "\n",
            "For Playbacks 1 and 2 the data environment is used by default. You can change these parameters to whatever you want if you do not wish to.\n",
            "\n",
            "Playbacks 1 & 2 Data Environment\n",
            "\n",
            "If you need to use a different data environment, you can create one which does not yet support this setting.\n",
            "\n",
            "Data Environment for Playbacks 1 and 2\n",
            "\n",
            "If you want to use a different Data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Example calculation of BLEU score\n",
        "reference = \"What data is used for Playbacks 1 and 2 in the Data environment?\".split()\n",
        "candidate = \"What data is used for Playbacks 1 and 2 in the Data environment?\".split()\n",
        "bleu_score = sentence_bleu([reference], candidate)\n",
        "print(f'BLEU score: {bleu_score}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwnzS4TiIMgx",
        "outputId": "f1b4f2c6-5d05-4df4-b3a0-c41e43bb117a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_inappropriate_content(text):\n",
        "    inappropriate_keywords = ['badword1', 'badword2']\n",
        "    for word in inappropriate_keywords:\n",
        "        if word in text:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "story = generate_story_gpt2(seed_text)\n",
        "if filter_inappropriate_content(story):\n",
        "    print(\"Inappropriate content detected.\")\n",
        "else:\n",
        "    print(story)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbZ4HvToIRl2",
        "outputId": "9ecb0a65-5e5d-4fb8-d5e7-3124f145b803"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What data is used for Playbacks 1 and 2 in the Data environment?\n",
            "\n",
            "There is a very large set of data used to determine Playback-related statistics for each game in a game on the system. Players are often asked to record and share playbacks. They are asked to perform the same tasks as their opponents. What is the reason for doing this data collection?\n",
            "\n",
            "The reason for performing this data collection is to obtain all of the playback data. We use that data to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9bSt6KGJDKf",
        "outputId": "ffe2de62-5b90-4e23-c3fb-6dbf554f93f4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.3.0+cu121)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.32.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.5.82)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.32.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.82)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "\n",
        "def fine_tune_model(train_file):\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "    dataset = TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=train_file,\n",
        "        block_size=128\n",
        "    )\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False,\n",
        "    )\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=100,\n",
        "        per_device_train_batch_size=32,\n",
        "        save_steps=10_000,\n",
        "        learning_rate=1e-4,  # Use a smaller learning rate\n",
        "        weight_decay=0.01,  # Add weight decay for regularization\n",
        "        save_total_limit=2,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=dataset,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    output_dir='./results'\n",
        "    # Save the model and tokenizer\n",
        "    model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "fine_tune_model('preprocessed_data.txt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "5HLiqg9IIfVi",
        "outputId": "50e4e033-7474-43bb-abbe-7a0fabee6a2b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [300/300 06:01, Epoch 100/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_story_structure(text):\n",
        "    return f\"<BOS> {text[:int(len(text)/3)]} <MID> {text[int(len(text)/3):int(2*len(text)/3)]} <EOS> {text[int(2*len(text)/3):]}\"\n",
        "\n",
        "structured_text = add_story_structure(\"What data is used for Playbacks 1 and 2 in the Data environment?...\")\n",
        "print(structured_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXHw1ShIKeqJ",
        "outputId": "14b05a26-d329-4b8f-b1be-e1c48216cbe4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<BOS> What data is used for  <MID> Playbacks 1 and 2 in t <EOS> he Data environment?...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    fp16=True,  # Enable mixed precision training\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        ")\n"
      ],
      "metadata": {
        "id": "ax8zipdKKfl8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Define the model name and load the tokenizer and model\n",
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('./results')  # Path to the fine-tuned model\n",
        "model = GPT2LMHeadModel.from_pretrained('./results')\n"
      ],
      "metadata": {
        "id": "KCdojTPGL449"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def generate_story(seed_text, max_length=100, top_k=50, top_p=0.95):\n",
        "    # Encode the input text\n",
        "    input_ids = tokenizer.encode(seed_text, return_tensors='pt')\n",
        "\n",
        "    # Generate text\n",
        "    sample_outputs = model.generate(\n",
        "        input_ids,\n",
        "        do_sample=True,\n",
        "        max_length=max_length,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "\n",
        "    # Decode the generated text\n",
        "    generated_text = tokenizer.decode(sample_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "def format_story(story, line_length=80):\n",
        "    words = story.split()\n",
        "    formatted_story = ''\n",
        "    line = ''\n",
        "    for word in words:\n",
        "        if len(line) + len(word) + 1 > line_length:\n",
        "            formatted_story += line.strip() + '\\n'\n",
        "            line = ''\n",
        "        line += word + ' '\n",
        "    formatted_story += line.strip()\n",
        "    return formatted_story\n",
        "\n",
        "# Example usage\n",
        "seed_text = \"What data is used for Playbacks 1 and 2 in the Data environment?\"\n",
        "story = generate_story(seed_text, max_length=512)\n",
        "formatted_story = format_story(story, line_length=80)\n",
        "print(formatted_story)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDJKeUIwL761",
        "outputId": "7af78a30-7465-4c4b-99bd-aaa2c2960ae6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What data is used for Playbacks 1 and 2 in the Data environment?the following\n",
            "table describes the types data used playbacks in data environment. purpose\n",
            "playbacks 1 2 development environment?the development environment used fix unit\n",
            "test defects found production testing activities. often development environment\n",
            "migrated test environment subject approval. often development environment\n",
            "migrated test environment subject approval. application used migrate test\n",
            "environment subject approval?the application used migrate test environment\n",
            "subject approval. often development environment migrated test environment\n",
            "unscrambled data performed parallel payroll environment?the development\n",
            "environment used migrate test environment unscrambled data performed parallel\n",
            "payroll environment highlighted blue figure 8 uat stage. data environment\n",
            "longer available?the data environment longer available 2027 hours period low\n",
            "usage minimize risk impact. main purpose test environment build stage?the test\n",
            "environment used testing activities often unit tested hrplus solution provide\n",
            "uptodate data support build stage. data environment longer available 2027 hours\n",
            "period?the data environment longer available 2027 hours period low usage\n",
            "minimize risk impact. main purpose data environment test environment build\n",
            "stage?the data environment used testing activities often unit tested hrplus\n",
            "solutions provide uptodate data support build stage. data environment longer\n",
            "available 2027 hours period low usage minimize risk impact. main purpose test\n",
            "environment build stage?the data environment used testing activities often\n",
            "single sign policies found testing activities. often single sign implemented\n",
            "test environment environment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainerCallback\n",
        "from tqdm import tqdm\n",
        "\n",
        "class ProgressBarCallback(TrainerCallback):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.epoch_pbar = None\n",
        "        self.step_pbar = None\n",
        "\n",
        "    def on_train_begin(self, args, state, control, **kwargs):\n",
        "        self.epoch_pbar = tqdm(total=args.num_train_epochs, desc=\"Epochs\")\n",
        "        self.step_pbar = tqdm(total=state.max_steps, desc=\"Steps\")\n",
        "\n",
        "    def on_epoch_end(self, args, state, control, **kwargs):\n",
        "        self.epoch_pbar.update(1)\n",
        "\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        self.step_pbar.update(1)\n",
        "\n",
        "    def on_train_end(self, args, state, control, **kwargs):\n",
        "        self.epoch_pbar.close()\n",
        "        self.step_pbar.close()\n"
      ],
      "metadata": {
        "id": "o4gizbX5Si-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "\n",
        "def fine_tune_model(train_file, model_name='gpt2', output_dir='./results'):\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "    dataset = TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=train_file,\n",
        "        block_size=128\n",
        "    )\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False,\n",
        "    )\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=100,\n",
        "        per_device_train_batch_size=64,\n",
        "        save_steps=10_000,\n",
        "        save_total_limit=2,\n",
        "        learning_rate=1e-4,  # Use a smaller learning rate\n",
        "        weight_decay=0.01,  # Add weight decay for regularization\n",
        "        warmup_ratio=0.1,  # Use a smaller warmup ratio\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=dataset,\n",
        "        callbacks=[ProgressBarCallback()]  # Add the custom progress bar callback\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the model and tokenizer\n",
        "    model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Fine-tune the model and save it\n",
        "fine_tune_model('preprocessed_data.txt')\n"
      ],
      "metadata": {
        "id": "1oFneWO7SmNf",
        "outputId": "d82ebb3a-5e35-45b6-83c4-eb7c88cbaef8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 998
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n",
            "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Steps:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "Steps:   0%|          | 1/200 [00:02<02:57,  1.12it/s]\u001b[A"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='48' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 48/200 00:41 < 02:17, 1.11 it/s, Epoch 23.50/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epochs:   1%|          | 1/100 [00:02<04:15,  2.58s/it]\n",
            "Steps:   2%|▏         | 3/200 [00:04<03:43,  1.13s/it]\u001b[A\n",
            "Epochs:   2%|▏         | 2/100 [00:04<03:45,  2.30s/it]\n",
            "Steps:   2%|▎         | 5/200 [00:05<03:29,  1.07s/it]\u001b[A\n",
            "Epochs:   3%|▎         | 3/100 [00:06<03:41,  2.28s/it]\n",
            "Steps:   4%|▎         | 7/200 [00:07<03:45,  1.17s/it]\u001b[A\n",
            "Epochs:   4%|▍         | 4/100 [00:08<03:02,  1.90s/it]\n",
            "Steps:   4%|▍         | 9/200 [00:09<02:58,  1.07it/s]\u001b[A\n",
            "Epochs:   5%|▌         | 5/100 [00:10<03:24,  2.15s/it]\n",
            "Steps:   6%|▌         | 11/200 [00:11<03:34,  1.13s/it]\u001b[A\n",
            "Epochs:   6%|▌         | 6/100 [00:12<03:18,  2.12s/it]\n",
            "Steps:   6%|▋         | 13/200 [00:13<03:31,  1.13s/it]\u001b[A\n",
            "Epochs:   7%|▋         | 7/100 [00:14<03:14,  2.09s/it]\n",
            "Steps:   8%|▊         | 15/200 [00:15<03:29,  1.13s/it]\u001b[A\n",
            "Epochs:   8%|▊         | 8/100 [00:16<02:49,  1.84s/it]\n",
            "Steps:   8%|▊         | 17/200 [00:17<02:48,  1.08it/s]\u001b[A\n",
            "Epochs:   9%|▉         | 9/100 [00:18<02:45,  1.82s/it]\n",
            "Steps:  10%|▉         | 19/200 [00:18<02:49,  1.07it/s]\u001b[A\n",
            "Epochs:  10%|█         | 10/100 [00:19<02:29,  1.66s/it]\n",
            "Steps:  10%|█         | 21/200 [00:20<02:28,  1.20it/s]\u001b[A\n",
            "Epochs:  11%|█         | 11/100 [00:20<02:19,  1.57s/it]\n",
            "Steps:  12%|█▏        | 23/200 [00:21<02:17,  1.28it/s]\u001b[A\n",
            "Epochs:  12%|█▏        | 12/100 [00:22<02:16,  1.55s/it]\n",
            "Steps:  12%|█▎        | 25/200 [00:23<02:21,  1.24it/s]\u001b[A\n",
            "Epochs:  13%|█▎        | 13/100 [00:23<02:11,  1.51s/it]\n",
            "Steps:  14%|█▎        | 27/200 [00:24<02:13,  1.29it/s]\u001b[A\n",
            "Epochs:  14%|█▍        | 14/100 [00:24<02:06,  1.47s/it]\n",
            "Steps:  14%|█▍        | 29/200 [00:25<02:08,  1.33it/s]\u001b[A\n",
            "Epochs:  15%|█▌        | 15/100 [00:26<02:14,  1.58s/it]\n",
            "Steps:  16%|█▌        | 31/200 [00:27<02:29,  1.13it/s]\u001b[A\n",
            "Epochs:  16%|█▌        | 16/100 [00:28<02:10,  1.55s/it]\n",
            "Steps:  16%|█▋        | 33/200 [00:29<02:19,  1.19it/s]\u001b[A\n",
            "Epochs:  17%|█▋        | 17/100 [00:30<02:19,  1.68s/it]\n",
            "Steps:  18%|█▊        | 35/200 [00:31<02:30,  1.10it/s]\u001b[A\n",
            "Epochs:  18%|█▊        | 18/100 [00:32<02:39,  1.94s/it]\n",
            "Steps:  18%|█▊        | 37/200 [00:33<03:04,  1.13s/it]\u001b[A\n",
            "Epochs:  19%|█▉        | 19/100 [00:34<02:38,  1.96s/it]\n",
            "Steps:  20%|█▉        | 39/200 [00:35<02:59,  1.11s/it]\u001b[A\n",
            "Epochs:  20%|██        | 20/100 [00:36<02:38,  1.98s/it]\n",
            "Steps:  20%|██        | 41/200 [00:37<02:52,  1.09s/it]\u001b[A\n",
            "Epochs:  21%|██        | 21/100 [00:38<02:20,  1.78s/it]\n",
            "Steps:  22%|██▏       | 43/200 [00:39<02:21,  1.11it/s]\u001b[A\n",
            "Epochs:  22%|██▏       | 22/100 [00:40<02:40,  2.05s/it]\n",
            "Steps:  22%|██▎       | 45/200 [00:41<02:57,  1.14s/it]\u001b[A\n",
            "Epochs:  23%|██▎       | 23/100 [00:42<02:37,  2.05s/it]\n",
            "Steps:  24%|██▎       | 47/200 [00:43<02:53,  1.14s/it]\u001b[A"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_training_metrics(training_args, trainer):\n",
        "    logs = trainer.state.log_history\n",
        "    steps = [log['step'] for log in logs if 'step' in log]\n",
        "    losses = [log['loss'] for log in logs if 'loss' in log]\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(steps, losses, label='Loss')\n",
        "    plt.xlabel('Steps')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss Over Time')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Plot training metrics after training\n",
        "plot_training_metrics(training_args, trainer)\n"
      ],
      "metadata": {
        "id": "xZrfZyg-SowY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}